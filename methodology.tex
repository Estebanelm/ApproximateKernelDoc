\chapter{Methodology}
\label{ch:metodologia}

The current project is a research on the effect of approximate computing 
techniques to the improvement of performance and energy consumption
on FPGAs. First, the research will be classified based on different perspectives. 
The classification will be
followed by the methods used on creating the CNNs and validating the
changes in execution time, energy consumption and accuracy degradation.

\section{Type of research}

The following classification is based on the different perspectives 
created by \cite{kumar2019research}.

\subsection{Application perspective}

This work is categorized as an applied research. 
The objective is to apply concepts from approximate computing theory
into another field of work or active research such as the study of CNNs on FPGAs.
These concepts will be mixed with other topics from computer architecture
and hardware generation.

\subsection{Objectives perspective}

From an objectives standpoint, this is an exploratory research due to trying to explore 
the possibilities of performance and energy gain through the use of approximate hardware generation.
The area of approximate computing on FPGAs, specifically for CNN implementations, is currently
underexplored. This research will help grow the field and could enable future research
or applications on using FPGAs to accelerate the field of machine learning.

\subsection{Mode of enquiry}

Finally, this is an unstructured research. The objective is to find how different techniques
affect the final measurements and tests in relation to exact kernels. The specific techniques
to be applied are not predetermined. As such, there is no expected accuracy value to be achieved. 
This project tries to 
push performance to the maximum and energy consumption
to the minimum while maintaining an acceptable accuracy, but there is no set number for
any of these values.

\section{Kernel creation}

The research is based on modifications done to an exact CNN implementation on an FPGA. 
The next sections specify the methods used to generate the kernels and apply the 
approximate modifications.

\subsection{Exact kernel generation}

The exact kernel generation will be based on the CaffeNet implementation. This is an AlexNet implementation
with the pooling and normalization layers reversed \cite{donahue2012bvlc}. 

The kernels are implemented using OpenCL on a DE1-SoC board, a hardware board with an on-chip processor and
a Cyclone V FPGA. Two codes must be generated, the host code to control the execution that will be executed on
the embedded chip of the board and the device code that will perform the CNN calculations.

The kernels already contain some approximations due to limitations on the FPGA being used, mainly on using
fixed-point values, as it does
not contain enough resources to implement a non-fixed-point implementation of the CaffeNet neural network.

\subsection{Approximate kernel generation}

The approximate kernels will be generated based on modifications done on the exact kernels.
Different techniques will be used to apply these modifications. Any modification must be tested
and different combinations of these modifications must be applied to show the change on accuracy,
performance and energy consumption.

The modifications must be applied on varying levels of abstraction, measuring the output on different
layers up to the output of the full CNN.

\section{Validation}

In this section, the validation to be used on the project is explained.

\subsection{Accuracy}

An algorithm based on the formulas found on \cite{googledev} is created using the Python. This algorithm
calculates accuracy of the approximate modifications against a Caffe \cite{jia2014caffe} based AlexNet implementation,
also written in Python.

The training weights to be used for evaluation are the ones found on \cite{donahue2012bvlc}. The original network
was trained using the Image-net Large Scale Visual Recognition Challenge 2012 image set \cite{lsvrc}.
The measurements will be done for top-1 and top-5 accuracy.

\subsection{Energy consumption}

For energy consumption, an initial measurement will be done with the FPGA doing no work, then after loading the program,
a new measurement will be done. This will be compared to the same measurement when loading the configuration with no
modifications done. This energy consumption will also be compared against the energy consumption of the computer
while running the Python implementation of CaffeNet.

\subsection{Performance}

Performance will be measured by a simple difference between the time 
at the end of the image classification and at the start.
This will be compared to the performance of the Caffe implementation 
and the exact implementation with no modifications.